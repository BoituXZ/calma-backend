# Model Configuration
MODEL_PATH=models/calma-final
BASE_MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct

# Inference Settings
MAX_TOKENS=256
TEMPERATURE=0.8
TOP_P=0.9
DO_SAMPLE=true

# Performance Settings
MAX_MEMORY_MB=8192
DEVICE_MAP=auto
TORCH_DTYPE=float16

# API Settings
HOST=0.0.0.0
PORT=8000
RELOAD=false

# CORS Settings (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:3001,http://localhost:8080,http://localhost:5173

# Service Information
SERVICE_NAME=Calma AI Inference Service
SERVICE_VERSION=1.0.0
MODEL_VERSION=calma-v1

# Timeout Settings (seconds)
INFERENCE_TIMEOUT=30
MODEL_LOAD_TIMEOUT=120

# Optional: Logging Configuration
LOG_LEVEL=INFO

# Optional: Monitoring and Metrics
# ENABLE_METRICS=false
# WANDB_PROJECT=calma-ai-inference
# WANDB_API_KEY=your_wandb_api_key_here